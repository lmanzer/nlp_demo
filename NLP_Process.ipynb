{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import string \n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import gensim\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "from src.scraping import extract_cnbc_article_info\n",
    "from src.urls import article_urls\n",
    "from src.nlp import lemmatize_words, remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape CNIB Website for Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "for article_url in article_urls:\n",
    "    _article_text_dict = extract_cnbc_article_info(article_url)\n",
    "    articles.append(_article_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article['title'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article['url']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Natural Language Processing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Article Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary '\\n'\n",
    "cleaned_article = article['article'].replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named-Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "nlp_data = nlp(cleaned_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp_data, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Entities\n",
    "entities =  nlp_data.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain top 10 people mentioned in article\n",
    "people = [ent.text.replace('\\n','') for ent in entities if ent.label_ == 'PERSON']\n",
    "top_people = Counter(people).most_common(10)\n",
    "\n",
    "top_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain top 10 organizations mentioned in article\n",
    "organizations = [ent.text.replace('\\n','') for ent in entities if ent.label_ == 'ORG']\n",
    "top_orgs = Counter(organizations).most_common(10)\n",
    "\n",
    "top_orgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize by Sentence\n",
    "article_sentence_list = nltk.tokenize.sent_tokenize(cleaned_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuation\n",
    "punctuation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "removed_punctuation = [w.translate(punctuation_table) for w in article_sentence_list]\n",
    "removed_mdash = [sentence.replace('â€”', '') for sentence in removed_punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words to lowercase\n",
    "lower_sentences = [s.lower() for s in removed_mdash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "stop_words =  nltk.corpus.stopwords.words('english')\n",
    "\n",
    "removed_stopwords = [remove_stopwords(sentence) for sentence in lower_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words to root word (e.g., 'according' to 'accord')\n",
    "lemmatize_sentences = [lemmatize_words(sentence) for sentence in removed_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for lemmatize_sentence, raw_sentence in zip(lemmatize_sentences, article_sentence_list):\n",
    "    \n",
    "    display(Markdown(f\"**Original**: {raw_sentence}\"))\n",
    "    display(Markdown(f\"**Cleaned**: {lemmatize_sentence}\"))\n",
    "    display(Markdown(f\"---\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Sentence Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = spacy.load('en_vectors_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Similarity Example\n",
    "X = ['cat', 'dog', 'banana', 'plantain', 'facebook', 'google']\n",
    "for word_i in X:\n",
    "    for word_j in X:\n",
    "            print(word_i, '-',word_j, ':', \n",
    "                  round(word_vectors(word_i).similarity(word_vectors(word_j)),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.zeros([len(lemmatize_sentences), \n",
    "                              len(lemmatize_sentences)])\n",
    "\n",
    "similarity_matrix\n",
    "for i, sentence_i in enumerate(lemmatize_sentences):\n",
    "    nlp_i = nlp(sentence_i)\n",
    "    for j, sentence_j in enumerate(lemmatize_sentences):\n",
    "        if i != j:\n",
    "            nlp_j = nlp(sentence_j)\n",
    "            similarity_matrix[i][j] = nlp_i.similarity(nlp_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impliment PageRank Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sentences = sorted((\n",
    "        (scores[i], sentence, article_sentence_list[i]) \n",
    "        for i,sentence in enumerate(lemmatize_sentences))\n",
    "        , reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SENTENCES = 3\n",
    "generated_summary = [ranked_sentence[2] \n",
    "                     for ranked_sentence \n",
    "                     in ranked_sentences[0:N_SENTENCES]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_demo",
   "language": "python",
   "name": "nlp_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

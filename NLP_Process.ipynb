{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import string \n",
    "\n",
    "import gensim\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "\n",
    "from src.scraping import extract_cnbc_article_info\n",
    "from src.nlp import nlp_analysis\n",
    "from src.urls import article_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape CNIB Website for Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = []\n",
    "for article_url in article_urls:\n",
    "    _article_text_dict = extract_cnbc_article_info(article_url)\n",
    "    article_list.append(_article_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = article_list[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Natural Language Processing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "# English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. \n",
    "# Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities.\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words =  nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "# GloVe is a pretrained word-vector \n",
    "#GloVe is an unsupervised learning algorithm for obtaining vector representations for words. \n",
    "# Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and \n",
    "#the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "word_embeddings = {}\n",
    "glove_model_path = os.path.join('data', 'models', 'external', 'glove', 'glove.6B.100d.txt')\n",
    "f = open(glove_model_path, encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_table = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Article Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_article = article['article'].replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named-Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_data = nlp(cleaned_article)\n",
    "entities =  nlp_data.ents\n",
    "\n",
    "organizations = [ent.text.replace('\\n','') for ent in entities if ent.label_ == 'ORG']\n",
    "people = [ent.text.replace('\\n','') for ent in entities if ent.label_ == 'PERSON']\n",
    "\n",
    "top_orgs = Counter(organizations).most_common(10)\n",
    "top_people = Counter(people).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Brnovich', 5), ('Mark Brnovich', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Facebook', 3), ('Google', 3), ('CNBC', 2), ('the Washington Post', 2)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_orgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence):\n",
    "    word_list = sentence.split()\n",
    "    _non_stopwords = \" \".join([word for word in word_list if word not in stop_words])\n",
    "    return _non_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(sentence):\n",
    "    # we can probably do better than this! \n",
    "    lemma_words = \" \".join([word.lemma_ for word in nlp(sentence)])\n",
    "    return lemma_words\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_sentence_list = nltk.tokenize.sent_tokenize(cleaned_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_punctuation = [w.translate(punctuation_table) for w in article_sentence_list]\n",
    "removed_mdash = [sentence.replace('â€”', '') for sentence in removed_punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_sentences = [s.lower() for s in removed_mdash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_stopwords = [remove_stopwords(sentence) for sentence in lower_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_sentences = [lemmatize_words(sentence) for sentence in removed_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sentence = lemmatize_sentences\n",
    "empty_vector = np.zeros((100,))\n",
    "sentence_vectors = []\n",
    "for sentence in final_sentence:\n",
    "    if len(sentence) > 0:\n",
    "        _word_list = sentence.split()\n",
    "        _word_vector =  [word_embeddings.get(word, empty_vector) for word in _word_list]\n",
    "        _summed_vector = sum(_word_vector)\n",
    "        normalized_vector = _summed_vector/ (len(sentence.split()))\n",
    "    else:\n",
    "        normalized_vector = empty_vector\n",
    "    sentence_vectors.append(normalized_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Similarity Matrix between Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = np.zeros([len(article_sentence_list), len(article_sentence_list)])\n",
    "\n",
    "for i in range(len(article_sentence_list)):\n",
    "    for j in range(len(article_sentence_list)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), \n",
    "                                              sentence_vectors[j].reshape(1,100))[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impliment PageRank Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sentences = sorted(((scores[i], s, article_sentence_list[i]) for i,s in enumerate(final_sentence)), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SENTENCES = 3\n",
    "generated_summary = [ranked_sentence[2] for ranked_sentence in ranked_sentences[0:N_SENTENCES]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Brnovich is one of several state attorneys general who spoke recently to the Washington Post about their willingness to take action against Facebook, Google and other tech giants, which they say have grown too powerful.',\n",
       " 'What\\'s being seen is the \"inaction or inability of the bureaucrats in Washington D.C. to do anything about protecting individual Americans, their privacy rights, how they are being manipulated when it comes to news feeds and news coverage,\" he said.',\n",
       " 'Google also gave the paper a statement that said, \"Privacy and security are built into all of our products, and we will continue to engage constructively with state attorneys general on policy issues.\"']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.tokenize.word_tokenize(sentence) for sentence in final_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arizona attorney general mark brnovich tell cnbc friday prepare go big tech company',\n",
       " '-PRON- s alone',\n",
       " 'tech company dominate market share essentially akin monopoly old brnovich say close bell',\n",
       " 'state ag take look maybe whether something do',\n",
       " 'brnovich one several state attorney general speak recently washington post willingness take action against facebook google tech giant say grow powerful',\n",
       " 'brnovich say worried massive amount datum collect manipulate',\n",
       " 'sometimes mislead maybe end maybe compromise privacy right',\n",
       " 'state step federal government be not say brnovich',\n",
       " 'washington dc least last decade good idea go die',\n",
       " 'what s see inaction inability bureaucrat washington dc anything protect individual american privacy right manipulate come news feed news coverage say',\n",
       " 'facebook google immediately respond cnbcs request comment',\n",
       " 'however statement washington post facebook vice president state local public policy say company productive conversation state ag',\n",
       " 'many official approach -PRON- constructive manner focused solution ensure company protect people information look forward work say',\n",
       " 'google also give paper statement say privacy security build product continue engage constructively state attorney general policy issue',\n",
       " 'brnovich would not comment individual company',\n",
       " 'assure matter big company violate right arizonian go take look go come hard courtroom appropriate']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dictionary = gensim.corpora.Dictionary(tokenized_sentences)  # This needs describing!\n",
    "article_corpus = [article_dictionary.doc2bow(text) for text in tokenized_sentences] # This needs describing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 5\n",
    "\n",
    "article_ldamodel = gensim.models.ldamodel.LdaModel(article_corpus, \n",
    "                                                   num_topics = NUM_TOPICS, \n",
    "                                                   id2word=article_dictionary, \n",
    "                                                   passes=15)\n",
    "topics = article_ldamodel.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.048*\"-PRON-\" + 0.026*\"protect\" + 0.026*\"say\" + 0.026*\"look\" + 0.026*\"company\"'),\n",
       " (1,\n",
       "  '0.040*\"say\" + 0.028*\"google\" + 0.028*\"privacy\" + 0.028*\"state\" + 0.028*\"news\"'),\n",
       " (2,\n",
       "  '0.039*\"go\" + 0.039*\"take\" + 0.027*\"big\" + 0.027*\"company\" + 0.027*\"brnovich\"'),\n",
       " (3,\n",
       "  '0.037*\"maybe\" + 0.037*\"state\" + 0.037*\"washington\" + 0.020*\"facebook\" + 0.020*\"ag\"'),\n",
       " (4,\n",
       "  '0.061*\"brnovich\" + 0.042*\"say\" + 0.042*\"company\" + 0.023*\"comment\" + 0.023*\"not\"')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_demo",
   "language": "python",
   "name": "nlp_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
